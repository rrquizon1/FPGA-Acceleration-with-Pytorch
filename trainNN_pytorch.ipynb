{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25jf_Y7zLEMe",
        "outputId": "7103ed60-5aee-4b3b-8ee2-09986fdbdd99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Current working directory: /content/drive/MyDrive/FPGA_Acce_pytorch\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change directory to a specific folder in Google Drive\n",
        "os.chdir('/content/drive/MyDrive/FPGA_Acce_pytorch')\n",
        "\n",
        "# Verify current working directory\n",
        "print(\"Current working directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3vm9AH4Ldho",
        "outputId": "95b94f31-82c2-490b-812e-cef9db0e2888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/FPGA_Acce_pytorch\n",
            " biasValues.h\t\t\t    'w_b (1)'\n",
            " data\t\t\t\t     w_b_test\n",
            " mnist_simple_nn.pth\t\t     weightsandbiases_final_128.txt\n",
            " mnist_simple_nn_with_accuracy.pth   weightsandbiases_final_2.txt\n",
            " sigContent.mif\t\t\t     weightsandbiases_final_normalized.txt\n",
            " testData\t\t\t     weightsandbiases_final.txt\n",
            " w_b\t\t\t\t     weightValues.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sigmoid Generation\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Oct  6 19:23:47 2025\n",
        "\n",
        "@author: Rhodz\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import math\n",
        "\n",
        "def genSigContent(dataWidth,sigmoidSize,weightIntSize,inputIntSize):\n",
        "    f = open(\"sigContent.mif\",\"w\")\n",
        "    fractBits = sigmoidSize-(weightIntSize+inputIntSize)\n",
        "    if fractBits < 0: #Sigmoid size is smaller the integer part of the MAC operation\n",
        "        fractBits = 0\n",
        "    x = -2**(weightIntSize+inputIntSize-1)#Smallest input going to the Sigmoid LUT from the neuron\n",
        "    for i in range(0,2**sigmoidSize):\n",
        "        y = sigmoid(x)\n",
        "        z = DtoB(y,dataWidth,dataWidth-inputIntSize)\n",
        "        print(y,z)\n",
        "        f.write(z+'\\n')\n",
        "        x=x+(2**-fractBits)\n",
        "    f.close()\n",
        "\n",
        "def DtoB(num,dataWidth,fracBits):#funtion for converting into two's complement format\n",
        "    if num >= 0:\n",
        "        num = num * (2**fracBits)\n",
        "        num = int(num)\n",
        "        e = bin(num)[2:]\n",
        "    else:\n",
        "        num = -num\n",
        "        num = num * (2**fracBits)#number of fractional bits\n",
        "        num = int(num)\n",
        "        if num == 0:\n",
        "            d = 0\n",
        "        else:\n",
        "            d = 2**dataWidth - num\n",
        "        e = bin(d)[2:]\n",
        "    return e\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    try:\n",
        "        return 1 / (1+math.exp(-x))#for x less than -1023 will give value error\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    genSigContent(dataWidth=16,sigmoidSize=10,weightIntSize=4,inputIntSize=1)\n",
        "#output sigmoid.mif is loaded for the sigmoid ROM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRZkIGrtLdkn",
        "outputId": "7ca78452-c004-4377-eb20-6b22e01fa3e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12535162055095e-07 0\n",
            "1.1610741114742279e-07 0\n",
            "1.1979305557162662e-07 0\n",
            "1.2359569488020953e-07 0\n",
            "1.275190428876251e-07 0\n",
            "1.3156693129733423e-07 0\n",
            "1.3574331344399593e-07 0\n",
            "1.4005226815444813e-07 0\n",
            "1.4449800373124837e-07 0\n",
            "1.4908486206266502e-07 0\n",
            "1.5381732286313315e-07 0\n",
            "1.587000080483163e-07 0\n",
            "1.63737686249047e-07 0\n",
            "1.6893527746855403e-07 0\n",
            "1.7429785788752586e-07 0\n",
            "1.7983066482170178e-07 0\n",
            "1.8553910183683314e-07 0\n",
            "1.914287440260102e-07 0\n",
            "1.9750534345450816e-07 0\n",
            "2.0377483477747004e-07 0\n",
            "2.1024334103591293e-07 0\n",
            "2.1691717963671816e-07 0\n",
            "2.238028685224453e-07 0\n",
            "2.3090713253699577e-07 0\n",
            "2.38236909993343e-07 0\n",
            "2.4579935944974304e-07 0\n",
            "2.5360186670104374e-07 0\n",
            "2.6165205199192004e-07 0\n",
            "2.6995777745908016e-07 0\n",
            "2.7852715480971087e-07 0\n",
            "2.8736855324366057e-07 0\n",
            "2.964906076270974e-07 0\n",
            "3.059022269256247e-07 0\n",
            "3.156126029050898e-07 0\n",
            "3.256312191085833e-07 0\n",
            "3.359678601183963e-07 0\n",
            "3.466326211119807e-07 0\n",
            "3.576359177212449e-07 0\n",
            "3.6898849620481354e-07 0\n",
            "3.8070144394318623e-07 0\n",
            "3.927862002670442e-07 0\n",
            "4.052545676292802e-07 0\n",
            "4.181187231316626e-07 0\n",
            "4.313912304173898e-07 0\n",
            "4.450850519411503e-07 0\n",
            "4.5921356162867124e-07 0\n",
            "4.73790557938118e-07 0\n",
            "4.888302773361011e-07 0\n",
            "5.043474082014517e-07 0\n",
            "5.203571051703427e-07 0\n",
            "5.368750039367665e-07 0\n",
            "5.53917236522822e-07 0\n",
            "5.715004470337245e-07 0\n",
            "5.896418079129241e-07 0\n",
            "6.083590367132095e-07 0\n",
            "6.276704134001712e-07 0\n",
            "6.475947982049267e-07 0\n",
            "6.681516500435412e-07 0\n",
            "6.8936104552113e-07 0\n",
            "7.112436985392046e-07 0\n",
            "7.338209805254081e-07 0\n",
            "7.571149413053982e-07 0\n",
            "7.811483306372579e-07 0\n",
            "8.059446204294677e-07 0\n",
            "8.315280276641321e-07 0\n",
            "8.579235380478508e-07 0\n",
            "8.851569304133282e-07 0\n",
            "9.132548018955539e-07 0\n",
            "9.422445939071362e-07 0\n",
            "9.721546189381598e-07 0\n",
            "1.0030140882067352e-06 0\n",
            "1.0348531401872454e-06 0\n",
            "1.067702870044147e-06 0\n",
            "1.101595360000069e-06 0\n",
            "1.136563710667868e-06 0\n",
            "1.1726420733772349e-06 0\n",
            "1.2098656835274182e-06 0\n",
            "1.2482708949986417e-06 0\n",
            "1.2878952156558145e-06 0\n",
            "1.3287773439792037e-06 0\n",
            "1.3709572068578448e-06 0\n",
            "1.4144759985825913e-06 0\n",
            "1.459376221076886e-06 0\n",
            "1.5057017254045332e-06 0\n",
            "1.553497754595012e-06 0\n",
            "1.6028109878281438e-06 0\n",
            "1.6536895860212646e-06 0\n",
            "1.7061832388634165e-06 0\n",
            "1.7603432133424856e-06 0\n",
            "1.816222403812674e-06 0\n",
            "1.8738753836511972e-06 0\n",
            "1.9333584585546463e-06 0\n",
            "1.994729721527057e-06 0\n",
            "2.0580491096133833e-06 0\n",
            "2.123378462433771e-06 0\n",
            "2.1907815825757865e-06 0\n",
            "2.2603242979035746e-06 0\n",
            "2.332074525844782e-06 0\n",
            "2.4061023397180246e-06 0\n",
            "2.4824800371656583e-06 0\n",
            "2.561282210758673e-06 0\n",
            "2.642585820842653e-06 0\n",
            "2.72647027069593e-06 0\n",
            "2.8130174840733106e-06 0\n",
            "2.902311985211097e-06 0\n",
            "2.9944409813715146e-06 0\n",
            "3.089494448007139e-06 0\n",
            "3.1875652166284775e-06 0\n",
            "3.288749065460498e-06 0\n",
            "3.393144812976612e-06 0\n",
            "3.5008544144014418e-06 0\n",
            "3.6119830612765793e-06 0\n",
            "3.726639284186561e-06 0\n",
            "3.84493505874534e-06 0\n",
            "3.966985914946731e-06 0\n",
            "4.0929110499856e-06 0\n",
            "4.222833444659923e-06 0\n",
            "4.3568799834673755e-06 0\n",
            "4.495181578513688e-06 0\n",
            "4.637873297353725e-06 0\n",
            "4.785094494890119e-06 0\n",
            "4.936988949458183e-06 0\n",
            "5.093705003229987e-06 0\n",
            "5.2553957070746375e-06 0\n",
            "5.422218970016178e-06 0\n",
            "5.594337713435001e-06 0\n",
            "5.77192003016332e-06 0\n",
            "5.955139348629957e-06 0\n",
            "6.144174602214718e-06 0\n",
            "6.339210403977635e-06 0\n",
            "6.540437226933636e-06 0\n",
            "6.748051590048605e-06 0\n",
            "6.9622562501383685e-06 0\n",
            "7.1832603998579244e-06 0\n",
            "7.411279871974134e-06 0\n",
            "7.646537350121257e-06 0\n",
            "7.889262586245034e-06 0\n",
            "8.139692624947503e-06 0\n",
            "8.39807203495153e-06 0\n",
            "8.664653147910923e-06 0\n",
            "8.939696304799167e-06 0\n",
            "9.223470110117277e-06 0\n",
            "9.516251694168764e-06 0\n",
            "9.818326983657703e-06 0\n",
            "1.0129990980873921e-05 0\n",
            "1.0451548051737734e-05 0\n",
            "1.0783312222985276e-05 0\n",
            "1.112560748878441e-05 0\n",
            "1.1478768127080351e-05 0\n",
            "1.1843139025979654e-05 0\n",
            "1.2219076020491007e-05 0\n",
            "1.2606946239951314e-05 0\n",
            "1.3007128466476033e-05 0\n",
            "1.342001350478341e-05 0\n",
            "1.3846004563753395e-05 0\n",
            "1.4285517650093407e-05 0\n",
            "1.4738981974494931e-05 0\n",
            "1.5206840370677114e-05 0\n",
            "1.5689549727726036e-05 0\n",
            "1.6187581436151334e-05 0\n",
            "1.670142184809518e-05 0\n",
            "1.723157275214239e-05 0\n",
            "1.7778551863194697e-05 0\n",
            "1.8342893327886845e-05 0\n",
            "1.8925148246037342e-05 0\n",
            "1.952588520864222e-05 0\n",
            "2.01456908529364e-05 0\n",
            "2.0785170435063725e-05 0\n",
            "2.144494842091395e-05 0\n",
            "2.212566909570261e-05 0\n",
            "2.2827997192887966e-05 0\n",
            "2.3552618543037957e-05 0\n",
            "2.43002407432796e-05 0\n",
            "2.5071593847983193e-05 0\n",
            "2.5867431081354405e-05 0\n",
            "2.668852957262856e-05 0\n",
            "2.7535691114583473e-05 0\n",
            "2.840974294610978e-05 0\n",
            "2.931153855960119e-05 0\n",
            "3.024195853395102e-05 0\n",
            "3.120191139396651e-05 1\n",
            "3.21923344970378e-05 1\n",
            "3.321419494792514e-05 1\n",
            "3.426849054255501e-05 1\n",
            "3.5356250741744315e-05 1\n",
            "3.6478537675800285e-05 1\n",
            "3.76364471809744e-05 1\n",
            "3.8831109868779e-05 1\n",
            "4.006369222920724e-05 1\n",
            "4.1335397768930325e-05 1\n",
            "4.264746818557913e-05 1\n",
            "4.4001184579253026e-05 1\n",
            "4.5397868702434395e-05 1\n",
            "4.6838884249524555e-05 1\n",
            "4.8325638187255363e-05 1\n",
            "4.98595821272702e-05 1\n",
            "5.144221374220898e-05 1\n",
            "5.307507822667364e-05 1\n",
            "5.4759769804494653e-05 1\n",
            "5.649793328376293e-05 1\n",
            "5.829126566113865e-05 1\n",
            "6.014151777699533e-05 1\n",
            "6.205049602300743e-05 10\n",
            "6.402006410383949e-05 10\n",
            "6.605214485464791e-05 10\n",
            "6.814872211615988e-05 10\n",
            "7.031184266914956e-05 10\n",
            "7.254361823018916e-05 10\n",
            "7.484622751061123e-05 10\n",
            "7.722191834067996e-05 10\n",
            "7.967300986103147e-05 10\n",
            "8.220189478350844e-05 10\n",
            "8.481104172358075e-05 10\n",
            "8.750299760661308e-05 10\n",
            "9.028039015031105e-05 10\n",
            "9.3145930425751e-05 11\n",
            "9.610241549947396e-05 11\n",
            "9.915273115920183e-05 11\n",
            "0.00010229985472581486 11\n",
            "0.00010554685795431097 11\n",
            "0.00010889691002655445 11\n",
            "0.00011235328063870752 11\n",
            "0.00011591934318633098 11\n",
            "0.00011959857805023158 11\n",
            "0.00012339457598623172 100\n",
            "0.00012731104162213555 100\n",
            "0.00013135179706526775 100\n",
            "0.0001355207856240677 100\n",
            "0.00013982207564732912 100\n",
            "0.0001442598644847886 100\n",
            "0.0001488384825728813 100\n",
            "0.0001535623976496012 101\n",
            "0.00015843621910252592 101\n",
            "0.00016346470245419303 101\n",
            "0.00016865275398914432 101\n",
            "0.0001740054355270893 101\n",
            "0.00017952796934677737 101\n",
            "0.0001852257432653108 110\n",
            "0.00019110431587777673 110\n",
            "0.00019716942196222918 110\n",
            "0.00020342697805520653 110\n",
            "0.00020988308820313111 110\n",
            "0.00021654404989510326 111\n",
            "0.0002234163601827719 111\n",
            "0.0002305067219931397 111\n",
            "0.00023782205064034186 111\n",
            "0.00024536948054262246 1000\n",
            "0.00025315637215092633 1000\n",
            "0.0002611903190957194 1000\n",
            "0.00026947915555885544 1000\n",
            "0.00027803096387751553 1001\n",
            "0.00028685408238746286 1001\n",
            "0.000295957113513077 1001\n",
            "0.0003053489321118595 1010\n",
            "0.0003150386940813393 1010\n",
            "0.0003250358452365473 1010\n",
            "0.0003353501304664781 1010\n",
            "0.00034599160317821547 1011\n",
            "0.0003569706350376584 1011\n",
            "0.0003682979260160596 1100\n",
            "0.0003799845147518645 1100\n",
            "0.00039204178923762814 1100\n",
            "0.00040448149784208087 1101\n",
            "0.00041731576067772006 1101\n",
            "0.0004305570813246149 1110\n",
            "0.00044421835892143437 1110\n",
            "0.00045831290063503817 1111\n",
            "0.0004728544345203107 1111\n",
            "0.0004878571227822659 1111\n",
            "0.0005033355754528114 10000\n",
            "0.0005193048644949271 10001\n",
            "0.0005357805383473942 10001\n",
            "0.0005527786369235996 10010\n",
            "0.0005703157070783359 10010\n",
            "0.0005884088185569387 10011\n",
            "0.0006070755804415096 10011\n",
            "0.0006263341581094202 10100\n",
            "0.000646203290719727 10101\n",
            "0.0006667023092435893 10101\n",
            "0.0006878511550552459 10110\n",
            "0.0007096703991005881 10111\n",
            "0.0007321812616608614 10111\n",
            "0.0007554056327295276 11000\n",
            "0.0007793660930208434 11001\n",
            "0.0008040859356292354 11010\n",
            "0.0008295891883591028 11011\n",
            "0.0008559006367452268 11100\n",
            "0.0008830458477845481 11100\n",
            "0.0009110511944006454 11101\n",
            "0.0009399438806628558 11110\n",
            "0.0009697519677825861 11111\n",
            "0.0010005044009099887 100000\n",
            "0.0010322310367548194 100001\n",
            "0.001064962672055945 100010\n",
            "0.001098731072924643 100100\n",
            "0.0011335690050875116 100101\n",
            "0.0011695102650555148 100110\n",
            "0.001206589712246389 100111\n",
            "0.0012448433020883745 101000\n",
            "0.0012843081201339695 101010\n",
            "0.0013250224172131609 101011\n",
            "0.0013670256456563559 101100\n",
            "0.001410358496618023 101110\n",
            "0.0014550629385328433 101111\n",
            "0.0015011822567369917 110001\n",
            "0.00154876109428798 110010\n",
            "0.0015978454940173438 110100\n",
            "0.0016484829418512883 110110\n",
            "0.001700722411435288 110111\n",
            "0.0017546144100994867 111001\n",
            "0.0018102110262026483 111011\n",
            "0.0018675659778932803 111101\n",
            "0.0019267346633274757 111111\n",
            "0.0019877742123839116 1000001\n",
            "0.002050743539917378 1000011\n",
            "0.002115703400593114 1000101\n",
            "0.0021827164453451808 1000111\n",
            "0.00225184727950302 1001001\n",
            "0.0023231625226312826 1001100\n",
            "0.0023967308701289603 1001110\n",
            "0.0024726231566347743 1010001\n",
            "0.002550912421286719 1010011\n",
            "0.0026316739748845795 1010110\n",
            "0.0027149854690051763 1011000\n",
            "0.0028009269671209736 1011011\n",
            "0.0028895810177736268 1011110\n",
            "0.0029810327298548972 1100001\n",
            "0.0030753698500482333 1100100\n",
            "0.0031726828424851893 1100111\n",
            "0.003273064970671631 1101011\n",
            "0.0033766123817395004 1101110\n",
            "0.003483424193080668 1110010\n",
            "0.00359360258142009 1110101\n",
            "0.0037072528743862213 1111001\n",
            "0.003824483644637211 1111101\n",
            "0.003945406806602022 10000001\n",
            "0.004070137715896128 10000101\n",
            "0.004198795271471868 10001001\n",
            "0.00433150202056399 10001101\n",
            "0.0044683842664911206 10010010\n",
            "0.004609572179374208 10010111\n",
            "0.004755199909833047 10011011\n",
            "0.004905405705722035 10100000\n",
            "0.005060332031966209 10100101\n",
            "0.005220125693558397 10101011\n",
            "0.0053849379617779605 10110000\n",
            "0.005554924703691103 10110110\n",
            "0.005730246514992078 10111011\n",
            "0.005911068856243796 11000001\n",
            "0.006097562192575309 11000111\n",
            "0.006289902136892483 11001110\n",
            "0.006488269596656705 11010100\n",
            "0.0066928509242848554 11011011\n",
            "0.006903838071221878 11100010\n",
            "0.007121428745735101 11101001\n",
            "0.007345826574477061 11110000\n",
            "0.007577241267860811 11111000\n",
            "0.007815888789288626 100000000\n",
            "0.008061991528271641 100001000\n",
            "0.008315778477474129 100010000\n",
            "0.008577485413711984 100011001\n",
            "0.008847355082930384 100100001\n",
            "0.00912563738918052 100101011\n",
            "0.009412589587609823 100110100\n",
            "0.009708476481474066 100111110\n",
            "0.010013570623173138 101001000\n",
            "0.010328152519305191 101010010\n",
            "0.010652510839726164 101011101\n",
            "0.01098694263059318 101101000\n",
            "0.011331753531361455 101110011\n",
            "0.011687257995694433 101111110\n",
            "0.012053779516236447 110001010\n",
            "0.01243165085318582 110010111\n",
            "0.012821214266594244 110100100\n",
            "0.01322282175230515 110110001\n",
            "0.013636835281429878 110111110\n",
            "0.014063627043245475 111001100\n",
            "0.014503579691381979 111011011\n",
            "0.01495708659314999 111101010\n",
            "0.015424552081841137 111111001\n",
            "0.015906391711814714 1000001001\n",
            "0.016403032516163058 1000011001\n",
            "0.01691491326672651 1000101010\n",
            "0.01744248473620534 1000111011\n",
            "0.01798620996209156 1001001101\n",
            "0.018546564512117298 1001011111\n",
            "0.019124036750888904 1001110010\n",
            "0.019719128107346592 1010000110\n",
            "0.020332353342658753 1010011010\n",
            "0.020964240818127502 1010101110\n",
            "0.021615332762647654 1011000100\n",
            "0.02228618553922549 1011011010\n",
            "0.022977369910025615 1011110000\n",
            "0.023689471299374428 1100001000\n",
            "0.02442309005410721 1100100000\n",
            "0.025178841700601858 1100111001\n",
            "0.02595735719779685 1101010010\n",
            "0.026759283185443152 1101101100\n",
            "0.027585282226789992 1110000111\n",
            "0.02843603304485266 1110100011\n",
            "0.02931223075135632 1111000000\n",
            "0.030214587067393973 1111011110\n",
            "0.03114383053477846 1111111100\n",
            "0.03210070671700818 10000011011\n",
            "0.033085978388704126 10000111100\n",
            "0.03410042571231152 10001011101\n",
            "0.03514484640079327 10001111111\n",
            "0.03622005586497474 10010100010\n",
            "0.03732688734412946 10011000111\n",
            "0.03846619201832429 10011101100\n",
            "0.03963883910097002 10100010010\n",
            "0.040845715909949315 10100111010\n",
            "0.042087727915618836 10101100011\n",
            "0.04336579876390677 10110001101\n",
            "0.044680870272650205 10110111000\n",
            "0.04603390239924038 10111100100\n",
            "0.04742587317756678 11000010010\n",
            "0.048857778622174906 11001000000\n",
            "0.05033063259747688 11001110001\n",
            "0.05184546664977984 11010100010\n",
            "0.05340332979982423 11011010101\n",
            "0.05500528829345414 11100001010\n",
            "0.05665242530797383 11101000000\n",
            "0.05834584061168106 11101110111\n",
            "0.060086650174007626 11110110000\n",
            "0.06187598572364272 11111101011\n",
            "0.06371499425196533 100000100111\n",
            "0.0656048374590693 100001100101\n",
            "0.0675466911396291 100010100101\n",
            "0.06954174450582809 100011100110\n",
            "0.07159119944455247 100100101001\n",
            "0.07369626970604846 100101101110\n",
            "0.07585818002124355 100110110101\n",
            "0.07807816514495095 100111111110\n",
            "0.08035746882220708 101001001001\n",
            "0.08269734267503885 101010010101\n",
            "0.08509904500702024 101011100100\n",
            "0.08756383952305867 101100110101\n",
            "0.09009299396195182 101110001000\n",
            "0.09268777863937604 101111011101\n",
            "0.09534946489910949 110000110100\n",
            "0.09807932347046003 110010001101\n",
            "0.10087862273005652 110011101001\n",
            "0.1037486268663797 110101000111\n",
            "0.10669059394565118 110110101000\n",
            "0.10970577387797076 111000001010\n",
            "0.11279540628289322 111001110000\n",
            "0.11596071825396675 111011010111\n",
            "0.11920292202211755 111101000010\n",
            "0.12252321251815919 111110101110\n",
            "0.12592276483513232 1000000011110\n",
            "0.12940273159163906 1000010010000\n",
            "0.13296424019782926 1000100000100\n",
            "0.13660839002621936 1000101111100\n",
            "0.1403362494900832 1000111110110\n",
            "0.14414885303274058 1001001110011\n",
            "0.14804719803168948 1001011110011\n",
            "0.15203224162217424 1001101110101\n",
            "0.1561048974454574 1001111111011\n",
            "0.1602660323277607 1010010000011\n",
            "0.16451646289656316 1010100001110\n",
            "0.16885695214168317 1010110011101\n",
            "0.1732882059293266 1011000101110\n",
            "0.17781086947804958 1011011000010\n",
            "0.18242552380635635 1011101011001\n",
            "0.18713268216242657 1011111110011\n",
            "0.19193278644723683 1100010010001\n",
            "0.19682620364309852 1100100110001\n",
            "0.20181322226037884 1100111010101\n",
            "0.2068940488158881 1101001111011\n",
            "0.21206880435710532 1101100100101\n",
            "0.2173375210470625 1101111010001\n",
            "0.22270013882530884 1110010000001\n",
            "0.22815650216092537 1110100110100\n",
            "0.2337063569140403 1110111101010\n",
            "0.23934934732271163 1111010100010\n",
            "0.24508501313237172 1111101011110\n",
            "0.25091278688527247 10000000011101\n",
            "0.25683199138751883 10000011011111\n",
            "0.26284183737131667 10000110100100\n",
            "0.2689414213699951 10001001101100\n",
            "0.2751297238231752 10001100110111\n",
            "0.28140560742914383 10010000000101\n",
            "0.2877678157610531 10010011010101\n",
            "0.29421497216298875 10010110101000\n",
            "0.3007455789412415 10011001111110\n",
            "0.30735801686526387 10011101010111\n",
            "0.31405054499180746 10100000110010\n",
            "0.320821300824607 10100100010000\n",
            "0.3276683008207139 10100111110001\n",
            "0.334589441253186 10101011010011\n",
            "0.341582499438317 10101110111000\n",
            "0.34864513533394575 10110010100000\n",
            "0.35577489351363034 10110110001010\n",
            "0.3629692055196168 10111001110101\n",
            "0.37022539259558657 10111101100011\n",
            "0.3775406687981454 11000001010011\n",
            "0.3849121444839335 11000101000100\n",
            "0.39233683016710835 11001000111000\n",
            "0.399811640739795 11001100101101\n",
            "0.40733340004593027 11010000100011\n",
            "0.4148988457967688 11010100011011\n",
            "0.4225046348141883 11011000010100\n",
            "0.43014734858584286 11011100001111\n",
            "0.43782349911420193 11100000001010\n",
            "0.4455295350395727 11100100000111\n",
            "0.45326184801538616 11101000000100\n",
            "0.461016779312316 11101100000010\n",
            "0.46879062662624377 11110000000001\n",
            "0.47657965106367606 11110100000000\n",
            "0.4843800842769844 11111000000000\n",
            "0.4921881357207956 11111100000000\n",
            "0.5 100000000000000\n",
            "0.5078118642792044 100000011111111\n",
            "0.5156199157230156 100000111111111\n",
            "0.523420348936324 100001011111111\n",
            "0.5312093733737563 100001111111110\n",
            "0.5389832206876841 100010011111101\n",
            "0.5467381519846138 100010111111011\n",
            "0.5544704649604273 100011011111000\n",
            "0.5621765008857981 100011111110101\n",
            "0.5698526514141571 100100011110000\n",
            "0.5774953651858118 100100111101011\n",
            "0.5851011542032312 100101011100100\n",
            "0.5926665999540697 100101111011100\n",
            "0.600188359260205 100110011010010\n",
            "0.6076631698328917 100110111000111\n",
            "0.6150878555160665 100111010111011\n",
            "0.6224593312018546 100111110101100\n",
            "0.6297746074044134 101000010011100\n",
            "0.6370307944803831 101000110001010\n",
            "0.6442251064863697 101001001110101\n",
            "0.6513548646660542 101001101011111\n",
            "0.658417500561683 101010001000111\n",
            "0.665410558746814 101010100101100\n",
            "0.672331699179286 101011000001110\n",
            "0.679178699175393 101011011101111\n",
            "0.6859494550081925 101011111001101\n",
            "0.6926419831347361 101100010101000\n",
            "0.6992544210587585 101100110000001\n",
            "0.7057850278370112 101101001010111\n",
            "0.712232184238947 101101100101010\n",
            "0.7185943925708561 101101111111010\n",
            "0.7248702761768248 101110011001000\n",
            "0.7310585786300049 101110110010011\n",
            "0.7371581626286834 101111001011011\n",
            "0.7431680086124811 101111100100000\n",
            "0.7490872131147275 101111111100010\n",
            "0.7549149868676283 110000010100001\n",
            "0.7606506526772884 110000101011101\n",
            "0.7662936430859597 110001000010101\n",
            "0.7718434978390747 110001011001011\n",
            "0.7772998611746911 110001101111110\n",
            "0.7826624789529376 110010000101110\n",
            "0.7879311956428947 110010011011010\n",
            "0.7931059511841119 110010110000100\n",
            "0.7981867777396212 110011000101010\n",
            "0.8031737963569016 110011011001110\n",
            "0.8080672135527632 110011101101110\n",
            "0.8128673178375735 110100000001100\n",
            "0.8175744761936437 110100010100110\n",
            "0.8221891305219503 110100100111101\n",
            "0.8267117940706734 110100111010001\n",
            "0.8311430478583168 110101001100010\n",
            "0.8354835371034369 110101011110001\n",
            "0.8397339676722393 110101101111100\n",
            "0.8438951025545426 110110000000100\n",
            "0.8479677583778257 110110010001010\n",
            "0.8519528019683106 110110100001100\n",
            "0.8558511469672594 110110110001100\n",
            "0.8596637505099167 110111000001001\n",
            "0.8633916099737806 110111010000011\n",
            "0.8670357598021706 110111011111011\n",
            "0.870597268408361 110111101101111\n",
            "0.8740772351648677 110111111100001\n",
            "0.8774767874818407 111000001010001\n",
            "0.8807970779778823 111000010111101\n",
            "0.8840392817460332 111000100101000\n",
            "0.8872045937171068 111000110001111\n",
            "0.8902942261220291 111000111110101\n",
            "0.8933094060543487 111001001010111\n",
            "0.8962513731336202 111001010111000\n",
            "0.8991213772699436 111001100010110\n",
            "0.90192067652954 111001101110010\n",
            "0.9046505351008906 111001111001011\n",
            "0.9073122213606241 111010000100010\n",
            "0.9099070060380482 111010001110111\n",
            "0.9124361604769414 111010011001010\n",
            "0.9149009549929797 111010100011011\n",
            "0.9173026573249612 111010101101010\n",
            "0.9196425311777929 111010110110110\n",
            "0.9219218348550491 111011000000001\n",
            "0.9241418199787566 111011001001010\n",
            "0.9263037302939515 111011010010001\n",
            "0.9284088005554476 111011011010110\n",
            "0.9304582554941719 111011100011001\n",
            "0.9324533088603709 111011101011010\n",
            "0.9343951625409306 111011110011010\n",
            "0.9362850057480346 111011111011000\n",
            "0.9381240142763573 111100000010100\n",
            "0.9399133498259924 111100001001111\n",
            "0.941654159388319 111100010001000\n",
            "0.9433475746920261 111100010111111\n",
            "0.9449947117065459 111100011110101\n",
            "0.9465966702001757 111100100101010\n",
            "0.94815453335022 111100101011101\n",
            "0.9496693674025232 111100110001110\n",
            "0.9511422213778251 111100110111111\n",
            "0.9525741268224334 111100111101101\n",
            "0.9539660976007597 111101000011011\n",
            "0.9553191297273498 111101001000111\n",
            "0.9566342012360932 111101001110010\n",
            "0.9579122720843811 111101010011100\n",
            "0.9591542840900507 111101011000101\n",
            "0.96036116089903 111101011101101\n",
            "0.9615338079816756 111101100010011\n",
            "0.9626731126558706 111101100111000\n",
            "0.9637799441350253 111101101011101\n",
            "0.9648551535992067 111101110000000\n",
            "0.9658995742876885 111101110100010\n",
            "0.9669140216112958 111101111000011\n",
            "0.9678992932829918 111101111100100\n",
            "0.9688561694652216 111110000000011\n",
            "0.9697854129326061 111110000100001\n",
            "0.9706877692486436 111110000111111\n",
            "0.9715639669551474 111110001011100\n",
            "0.9724147177732099 111110001111000\n",
            "0.9732407168145568 111110010010011\n",
            "0.9740426428022031 111110010101101\n",
            "0.9748211582993981 111110011000110\n",
            "0.9755769099458929 111110011011111\n",
            "0.9763105287006256 111110011110111\n",
            "0.9770226300899744 111110100001111\n",
            "0.9777138144607745 111110100100101\n",
            "0.9783846672373524 111110100111011\n",
            "0.9790357591818724 111110101010001\n",
            "0.9796676466573412 111110101100101\n",
            "0.9802808718926534 111110101111001\n",
            "0.9808759632491112 111110110001101\n",
            "0.9814534354878828 111110110100000\n",
            "0.9820137900379085 111110110110010\n",
            "0.9825575152637948 111110111000100\n",
            "0.9830850867332734 111110111010101\n",
            "0.983596967483837 111110111100110\n",
            "0.9840936082881853 111110111110110\n",
            "0.9845754479181588 111111000000110\n",
            "0.98504291340685 111111000010101\n",
            "0.985496420308618 111111000100100\n",
            "0.9859363729567544 111111000110011\n",
            "0.9863631647185701 111111001000001\n",
            "0.9867771782476948 111111001001110\n",
            "0.9871787857334058 111111001011011\n",
            "0.9875683491468141 111111001101000\n",
            "0.9879462204837635 111111001110101\n",
            "0.9883127420043056 111111010000001\n",
            "0.9886682464686385 111111010001100\n",
            "0.9890130573694068 111111010010111\n",
            "0.9893474891602738 111111010100010\n",
            "0.9896718474806949 111111010101101\n",
            "0.9899864293768268 111111010110111\n",
            "0.9902915235185259 111111011000001\n",
            "0.9905874104123901 111111011001011\n",
            "0.9908743626108194 111111011010100\n",
            "0.9911526449170697 111111011011110\n",
            "0.991422514586288 111111011100110\n",
            "0.9916842215225259 111111011101111\n",
            "0.9919380084717284 111111011110111\n",
            "0.9921841112107114 111111011111111\n",
            "0.9924227587321393 111111100000111\n",
            "0.9926541734255229 111111100001111\n",
            "0.9928785712542649 111111100010110\n",
            "0.993096161928778 111111100011101\n",
            "0.9933071490757153 111111100100100\n",
            "0.9935117304033434 111111100101011\n",
            "0.9937100978631075 111111100110001\n",
            "0.9939024378074247 111111100111000\n",
            "0.9940889311437562 111111100111110\n",
            "0.994269753485008 111111101000100\n",
            "0.994445075296309 111111101001001\n",
            "0.9946150620382221 111111101001111\n",
            "0.9947798743064417 111111101010100\n",
            "0.9949396679680339 111111101011010\n",
            "0.9950945942942779 111111101011111\n",
            "0.9952448000901669 111111101100100\n",
            "0.9953904278206259 111111101101000\n",
            "0.9955316157335089 111111101101101\n",
            "0.9956684979794361 111111101110010\n",
            "0.9958012047285281 111111101110110\n",
            "0.995929862284104 111111101111010\n",
            "0.9960545931933981 111111101111110\n",
            "0.9961755163553627 111111110000010\n",
            "0.9962927471256138 111111110000110\n",
            "0.9964063974185798 111111110001010\n",
            "0.9965165758069192 111111110001101\n",
            "0.9966233876182606 111111110010001\n",
            "0.9967269350293284 111111110010100\n",
            "0.9968273171575148 111111110011000\n",
            "0.9969246301499518 111111110011011\n",
            "0.9970189672701452 111111110011110\n",
            "0.9971104189822263 111111110100001\n",
            "0.9971990730328789 111111110100100\n",
            "0.9972850145309949 111111110100111\n",
            "0.9973683260251155 111111110101001\n",
            "0.9974490875787134 111111110101100\n",
            "0.9975273768433653 111111110101110\n",
            "0.997603269129871 111111110110001\n",
            "0.9976768374773688 111111110110011\n",
            "0.997748152720497 111111110110110\n",
            "0.9978172835546547 111111110111000\n",
            "0.9978842965994068 111111110111010\n",
            "0.9979492564600826 111111110111100\n",
            "0.998012225787616 111111110111110\n",
            "0.9980732653366725 111111111000000\n",
            "0.9981324340221067 111111111000010\n",
            "0.9981897889737974 111111111000100\n",
            "0.9982453855899006 111111111000110\n",
            "0.9982992775885648 111111111001000\n",
            "0.9983515170581486 111111111001001\n",
            "0.9984021545059827 111111111001011\n",
            "0.998451238905712 111111111001101\n",
            "0.998498817743263 111111111001110\n",
            "0.9985449370614672 111111111010000\n",
            "0.9985896415033819 111111111010001\n",
            "0.9986329743543437 111111111010011\n",
            "0.9986749775827868 111111111010100\n",
            "0.9987156918798661 111111111010101\n",
            "0.9987551566979116 111111111010111\n",
            "0.9987934102877537 111111111011000\n",
            "0.9988304897349445 111111111011001\n",
            "0.9988664309949126 111111111011010\n",
            "0.9989012689270753 111111111011011\n",
            "0.9989350373279441 111111111011101\n",
            "0.9989677689632452 111111111011110\n",
            "0.99899949559909 111111111011111\n",
            "0.9990302480322174 111111111100000\n",
            "0.9990600561193372 111111111100001\n",
            "0.9990889488055994 111111111100010\n",
            "0.9991169541522155 111111111100011\n",
            "0.9991440993632549 111111111100011\n",
            "0.9991704108116409 111111111100100\n",
            "0.9991959140643708 111111111100101\n",
            "0.9992206339069791 111111111100110\n",
            "0.9992445943672705 111111111100111\n",
            "0.9992678187383391 111111111101000\n",
            "0.9992903296008995 111111111101000\n",
            "0.9993121488449448 111111111101001\n",
            "0.9993332976907565 111111111101010\n",
            "0.9993537967092804 111111111101010\n",
            "0.9993736658418905 111111111101011\n",
            "0.9993929244195585 111111111101100\n",
            "0.9994115911814431 111111111101100\n",
            "0.9994296842929216 111111111101101\n",
            "0.9994472213630764 111111111101101\n",
            "0.9994642194616526 111111111101110\n",
            "0.999480695135505 111111111101110\n",
            "0.9994966644245472 111111111101111\n",
            "0.9995121428772178 111111111110000\n",
            "0.9995271455654797 111111111110000\n",
            "0.999541687099365 111111111110000\n",
            "0.9995557816410785 111111111110001\n",
            "0.9995694429186754 111111111110001\n",
            "0.9995826842393223 111111111110010\n",
            "0.9995955185021579 111111111110010\n",
            "0.9996079582107623 111111111110011\n",
            "0.999620015485248 111111111110011\n",
            "0.999631702073984 111111111110011\n",
            "0.9996430293649623 111111111110100\n",
            "0.9996540083968218 111111111110100\n",
            "0.9996646498695336 111111111110101\n",
            "0.9996749641547633 111111111110101\n",
            "0.9996849613059188 111111111110101\n",
            "0.999694651067888 111111111110101\n",
            "0.999704042886487 111111111110110\n",
            "0.9997131459176125 111111111110110\n",
            "0.9997219690361224 111111111110110\n",
            "0.9997305208444413 111111111110111\n",
            "0.9997388096809043 111111111110111\n",
            "0.9997468436278489 111111111110111\n",
            "0.9997546305194572 111111111110111\n",
            "0.9997621779493597 111111111111000\n",
            "0.9997694932780068 111111111111000\n",
            "0.9997765836398173 111111111111000\n",
            "0.9997834559501049 111111111111000\n",
            "0.9997901169117969 111111111111001\n",
            "0.9997965730219448 111111111111001\n",
            "0.9998028305780378 111111111111001\n",
            "0.9998088956841223 111111111111001\n",
            "0.9998147742567347 111111111111001\n",
            "0.9998204720306533 111111111111010\n",
            "0.9998259945644729 111111111111010\n",
            "0.9998313472460109 111111111111010\n",
            "0.9998365352975459 111111111111010\n",
            "0.9998415637808975 111111111111010\n",
            "0.9998464376023504 111111111111010\n",
            "0.999851161517427 111111111111011\n",
            "0.9998557401355151 111111111111011\n",
            "0.9998601779243528 111111111111011\n",
            "0.9998644792143758 111111111111011\n",
            "0.9998686482029348 111111111111011\n",
            "0.9998726889583779 111111111111011\n",
            "0.9998766054240137 111111111111011\n",
            "0.9998804014219498 111111111111100\n",
            "0.9998840806568136 111111111111100\n",
            "0.9998876467193613 111111111111100\n",
            "0.9998911030899734 111111111111100\n",
            "0.9998944531420457 111111111111100\n",
            "0.9998977001452741 111111111111100\n",
            "0.9999008472688409 111111111111100\n",
            "0.9999038975845005 111111111111100\n",
            "0.9999068540695742 111111111111100\n",
            "0.9999097196098496 111111111111101\n",
            "0.9999124970023934 111111111111101\n",
            "0.9999151889582765 111111111111101\n",
            "0.9999177981052164 111111111111101\n",
            "0.9999203269901391 111111111111101\n",
            "0.9999227780816594 111111111111101\n",
            "0.9999251537724895 111111111111101\n",
            "0.9999274563817697 111111111111101\n",
            "0.9999296881573309 111111111111101\n",
            "0.9999318512778838 111111111111101\n",
            "0.9999339478551454 111111111111101\n",
            "0.9999359799358962 111111111111101\n",
            "0.9999379495039771 111111111111101\n",
            "0.9999398584822231 111111111111110\n",
            "0.9999417087343389 111111111111110\n",
            "0.9999435020667163 111111111111110\n",
            "0.9999452402301955 111111111111110\n",
            "0.9999469249217734 111111111111110\n",
            "0.9999485577862579 111111111111110\n",
            "0.9999501404178727 111111111111110\n",
            "0.9999516743618128 111111111111110\n",
            "0.9999531611157506 111111111111110\n",
            "0.9999546021312976 111111111111110\n",
            "0.9999559988154207 111111111111110\n",
            "0.9999573525318144 111111111111110\n",
            "0.999958664602231 111111111111110\n",
            "0.9999599363077709 111111111111110\n",
            "0.9999611688901312 111111111111110\n",
            "0.9999623635528191 111111111111110\n",
            "0.9999635214623243 111111111111110\n",
            "0.9999646437492582 111111111111110\n",
            "0.9999657315094574 111111111111110\n",
            "0.9999667858050522 111111111111110\n",
            "0.9999678076655029 111111111111110\n",
            "0.9999687980886061 111111111111110\n",
            "0.9999697580414662 111111111111111\n",
            "0.9999706884614404 111111111111111\n",
            "0.999971590257054 111111111111111\n",
            "0.9999724643088853 111111111111111\n",
            "0.9999733114704275 111111111111111\n",
            "0.9999741325689185 111111111111111\n",
            "0.999974928406152 111111111111111\n",
            "0.9999756997592568 111111111111111\n",
            "0.999976447381457 111111111111111\n",
            "0.9999771720028072 111111111111111\n",
            "0.9999778743309043 111111111111111\n",
            "0.9999785550515792 111111111111111\n",
            "0.9999792148295649 111111111111111\n",
            "0.999979854309147 111111111111111\n",
            "0.9999804741147913 111111111111111\n",
            "0.999981074851754 111111111111111\n",
            "0.9999816571066721 111111111111111\n",
            "0.9999822214481369 111111111111111\n",
            "0.999982768427248 111111111111111\n",
            "0.999983298578152 111111111111111\n",
            "0.9999838124185639 111111111111111\n",
            "0.9999843104502723 111111111111111\n",
            "0.9999847931596293 111111111111111\n",
            "0.9999852610180254 111111111111111\n",
            "0.9999857144823499 111111111111111\n",
            "0.9999861539954363 111111111111111\n",
            "0.9999865799864952 111111111111111\n",
            "0.9999869928715335 111111111111111\n",
            "0.9999873930537599 111111111111111\n",
            "0.9999877809239794 111111111111111\n",
            "0.9999881568609741 111111111111111\n",
            "0.999988521231873 111111111111111\n",
            "0.9999888743925112 111111111111111\n",
            "0.999989216687777 111111111111111\n",
            "0.9999895484519483 111111111111111\n",
            "0.9999898700090192 111111111111111\n",
            "0.9999901816730163 111111111111111\n",
            "0.9999904837483059 111111111111111\n",
            "0.9999907765298898 111111111111111\n",
            "0.9999910603036951 111111111111111\n",
            "0.9999913353468521 111111111111111\n",
            "0.9999916019279651 111111111111111\n",
            "0.9999918603073751 111111111111111\n",
            "0.9999921107374138 111111111111111\n",
            "0.9999923534626499 111111111111111\n",
            "0.9999925887201281 111111111111111\n",
            "0.9999928167396002 111111111111111\n",
            "0.9999930377437499 111111111111111\n",
            "0.9999932519484098 111111111111111\n",
            "0.9999934595627731 111111111111111\n",
            "0.9999936607895961 111111111111111\n",
            "0.9999938558253978 111111111111111\n",
            "0.9999940448606512 111111111111111\n",
            "0.9999942280799697 111111111111111\n",
            "0.9999944056622865 111111111111111\n",
            "0.9999945777810301 111111111111111\n",
            "0.999994744604293 111111111111111\n",
            "0.9999949062949967 111111111111111\n",
            "0.9999950630110505 111111111111111\n",
            "0.9999952149055051 111111111111111\n",
            "0.9999953621267027 111111111111111\n",
            "0.9999955048184215 111111111111111\n",
            "0.9999956431200164 111111111111111\n",
            "0.9999957771665553 111111111111111\n",
            "0.99999590708895 111111111111111\n",
            "0.999996033014085 111111111111111\n",
            "0.9999961550649413 111111111111111\n",
            "0.9999962733607158 111111111111111\n",
            "0.9999963880169387 111111111111111\n",
            "0.9999964991455856 111111111111111\n",
            "0.9999966068551871 111111111111111\n",
            "0.9999967112509346 111111111111111\n",
            "0.9999968124347834 111111111111111\n",
            "0.999996910505552 111111111111111\n",
            "0.9999970055590187 111111111111111\n",
            "0.9999970976880148 111111111111111\n",
            "0.999997186982516 111111111111111\n",
            "0.9999972735297293 111111111111111\n",
            "0.9999973574141792 111111111111111\n",
            "0.9999974387177893 111111111111111\n",
            "0.9999975175199628 111111111111111\n",
            "0.9999975938976604 111111111111111\n",
            "0.9999976679254741 111111111111111\n",
            "0.999997739675702 111111111111111\n",
            "0.9999978092184174 111111111111111\n",
            "0.9999978766215375 111111111111111\n",
            "0.9999979419508905 111111111111111\n",
            "0.9999980052702785 111111111111111\n",
            "0.9999980666415416 111111111111111\n",
            "0.9999981261246162 111111111111111\n",
            "0.9999981837775962 111111111111111\n",
            "0.9999982396567868 111111111111111\n",
            "0.9999982938167612 111111111111111\n",
            "0.9999983463104141 111111111111111\n",
            "0.9999983971890123 111111111111111\n",
            "0.9999984465022453 111111111111111\n",
            "0.9999984942982746 111111111111111\n",
            "0.9999985406237789 111111111111111\n",
            "0.9999985855240013 111111111111111\n",
            "0.999998629042793 111111111111111\n",
            "0.9999986712226561 111111111111111\n",
            "0.9999987121047844 111111111111111\n",
            "0.999998751729105 111111111111111\n",
            "0.9999987901343165 111111111111111\n",
            "0.9999988273579266 111111111111111\n",
            "0.9999988634362894 111111111111111\n",
            "0.99999889840464 111111111111111\n",
            "0.9999989322971299 111111111111111\n",
            "0.9999989651468597 111111111111111\n",
            "0.9999989969859118 111111111111111\n",
            "0.9999990278453811 111111111111111\n",
            "0.9999990577554062 111111111111111\n",
            "0.999999086745198 111111111111111\n",
            "0.9999991148430695 111111111111111\n",
            "0.999999142076462 111111111111111\n",
            "0.9999991684719722 111111111111111\n",
            "0.9999991940553795 111111111111111\n",
            "0.9999992188516693 111111111111111\n",
            "0.9999992428850587 111111111111111\n",
            "0.9999992661790194 111111111111111\n",
            "0.9999992887563014 111111111111111\n",
            "0.9999993106389544 111111111111111\n",
            "0.9999993318483499 111111111111111\n",
            "0.9999993524052017 111111111111111\n",
            "0.9999993723295867 111111111111111\n",
            "0.9999993916409633 111111111111111\n",
            "0.9999994103581922 111111111111111\n",
            "0.999999428499553 111111111111111\n",
            "0.9999994460827635 111111111111111\n",
            "0.999999463124996 111111111111111\n",
            "0.9999994796428949 111111111111111\n",
            "0.9999994956525918 111111111111111\n",
            "0.9999995111697226 111111111111111\n",
            "0.9999995262094421 111111111111111\n",
            "0.9999995407864383 111111111111111\n",
            "0.9999995549149481 111111111111111\n",
            "0.9999995686087695 111111111111111\n",
            "0.9999995818812769 111111111111111\n",
            "0.9999995947454324 111111111111111\n",
            "0.9999996072137998 111111111111111\n",
            "0.999999619298556 111111111111111\n",
            "0.9999996310115038 111111111111111\n",
            "0.9999996423640822 111111111111111\n",
            "0.999999653367379 111111111111111\n",
            "0.9999996640321399 111111111111111\n",
            "0.9999996743687808 111111111111111\n",
            "0.9999996843873972 111111111111111\n",
            "0.999999694097773 111111111111111\n",
            "0.9999997035093924 111111111111111\n",
            "0.9999997126314468 111111111111111\n",
            "0.9999997214728452 111111111111111\n",
            "0.9999997300422225 111111111111111\n",
            "0.9999997383479481 111111111111111\n",
            "0.9999997463981333 111111111111111\n",
            "0.9999997542006405 111111111111111\n",
            "0.9999997617630899 111111111111111\n",
            "0.9999997690928675 111111111111111\n",
            "0.9999997761971314 111111111111111\n",
            "0.9999997830828204 111111111111111\n",
            "0.999999789756659 111111111111111\n",
            "0.9999997962251652 111111111111111\n",
            "0.9999998024946565 111111111111111\n",
            "0.999999808571256 111111111111111\n",
            "0.9999998144608981 111111111111111\n",
            "0.9999998201693351 111111111111111\n",
            "0.9999998257021421 111111111111111\n",
            "0.9999998310647226 111111111111111\n",
            "0.9999998362623136 111111111111111\n",
            "0.9999998412999919 111111111111111\n",
            "0.9999998461826772 111111111111111\n",
            "0.999999850915138 111111111111111\n",
            "0.9999998555019962 111111111111111\n",
            "0.9999998599477319 111111111111111\n",
            "0.9999998642566865 111111111111111\n",
            "0.9999998684330687 111111111111111\n",
            "0.9999998724809571 111111111111111\n",
            "0.9999998764043052 111111111111111\n",
            "0.9999998802069444 111111111111111\n",
            "0.9999998838925889 111111111111111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# simple_mnist_nn.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 1. Load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),                     # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # Normalize using MNIST mean and std\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "test_dataset  = datasets.MNIST(root=\"./data\", train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
      ],
      "metadata": {
        "id": "xeYBq8U1Ldng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOptXSwEbTbZ",
        "outputId": "e7758e02-ffcc-43bf-8ad3-25aa7f537212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define a simple neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "       # self.fc3 = nn.Linear(30, 10)\n",
        "        #self.fc4 = nn.Linear(10, 10)\n",
        "\n",
        "        # Xavier init (like Keras)\n",
        "        for layer in [self.fc1, self.fc2]:\n",
        "            nn.init.xavier_uniform_(layer.weight)\n",
        "            nn.init.zeros_(layer.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        #x = torch.sigmoid(self.fc2(x))\n",
        "        #x = torch.sigmoid(self.fc3(x))\n",
        "        x = self.fc2(x)   # raw logits for CrossEntropyLoss\n",
        "        return x\n",
        "\n",
        "model = SimpleNN().to(device)\n",
        "\n",
        "# 3. Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 4. Training loop with accuracy tracking\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # compute training accuracy\n",
        "        preds = output.argmax(dim=1)\n",
        "        correct += preds.eq(target).sum().item()\n",
        "        total += target.size(0)\n",
        "\n",
        "    train_acc = 100. * correct / total\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {avg_loss:.4f} - Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "# 5. Test the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += pred.eq(target).sum().item()\n",
        "\n",
        "test_acc = 100. * correct / len(test_loader.dataset)\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "# 6. Save model\n",
        "torch.save(model.state_dict(), \"mnist_simple_nn_with_accuracy.pth\")\n",
        "print(\"Model saved as mnist_simple_nn_with_accuracy.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObNHS3NFLp3o",
        "outputId": "b036617a-d68c-40d1-c761-59fd211d9004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Loss: 0.3449 - Train Acc: 90.70%\n",
            "Epoch 2/5 - Loss: 0.1711 - Train Acc: 95.13%\n",
            "Epoch 3/5 - Loss: 0.1257 - Train Acc: 96.47%\n",
            "Epoch 4/5 - Loss: 0.0978 - Train Acc: 97.29%\n",
            "Epoch 5/5 - Loss: 0.0781 - Train Acc: 97.86%\n",
            "\n",
            "Final Test Accuracy: 97.32%\n",
            "Model saved as mnist_simple_nn_with_accuracy.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Test the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += pred.eq(target).sum().item()\n",
        "\n",
        "accuracy = 100. * correct / len(test_loader.dataset)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 6. Save the trained model\n",
        "torch.save(model.state_dict(), \"mnist_simple_nn.pth\")\n",
        "print(\"Model saved as mnist_simple_nn.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXEEWu_bLssw",
        "outputId": "26059dcb-e5f9-4fae-86ba-32f0b0aa6bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 97.32%\n",
            "Model saved as mnist_simple_nn.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract weights and biases\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "weightList = []\n",
        "biasList = []\n",
        "\n",
        "# Extract weights and biases from all Linear layers\n",
        "for layer in model.children():\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        weights = layer.weight.detach().cpu().numpy().tolist()  # shape [out_features, in_features]\n",
        "        biases  = layer.bias.detach().cpu().numpy().reshape(-1, 1).tolist()  # shape [out_features, 1]\n",
        "\n",
        "        # Transpose weights to match Keras-style orientation\n",
        "        weightList.append(list(map(list, zip(*weights))))\n",
        "        biasList.append(biases)\n",
        "\n",
        "# Combine into dictionary\n",
        "data = {\"weights\": weightList, \"biases\": biasList}\n",
        "\n",
        "# Save to text file for FPGA post-processing\n",
        "with open(\"weightsandbiases_final_128_normalized.txt\", \"w\") as f:\n",
        "    json.dump(data, f)\n",
        "\n",
        "print(\" Weights and biases exported to weightsandbiases_final_128_normalized.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qplvHn2SOHY",
        "outputId": "0bc7d86b-fef6-42f5-dd9a-1e8af97ced46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Weights and biases exported to weightsandbiases_final_128_normalized.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img, label = train_dataset[0]\n",
        "print(f\"Image shape: {img.shape}, Label: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_z9vZUdMlDA",
        "outputId": "8acd9683-4dc3-4589-c95f-75f8547133dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: torch.Size([1, 28, 28]), Label: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Weights and Biases conversion to .if file\n",
        "import json\n",
        "\n",
        "dataWidth = 16\n",
        "dataIntWidth = 1\n",
        "weightIntWidth = 4\n",
        "inputFile = \"weightsandbiases_final_128.txt\" # output of the previous cell\n",
        "dataFracWidth = dataWidth-dataIntWidth\n",
        "weightFracWidth = dataWidth-weightIntWidth\n",
        "biasIntWidth = dataIntWidth+weightIntWidth\n",
        "biasFracWidth = dataWidth-biasIntWidth\n",
        "outputPath = \"./w_b/\" # location of weights and biases files. Make this folder manually\n",
        "headerPath = \"./\"\n",
        "\n",
        "def DtoB(num,dataWidth,fracBits):\t\t\t\t\t\t#funtion for converting into two's complement format\n",
        "\tif num >= 0:\n",
        "\t\tnum = num * (2**fracBits)\n",
        "\t\tnum = int(num)\n",
        "\t\td = num\n",
        "\telse:\n",
        "\t\tnum = -num\n",
        "\t\tnum = num * (2**fracBits)\t\t#number of fractional bits\n",
        "\t\tnum = int(num)\n",
        "\t\tif num == 0:\n",
        "\t\t\td = 0\n",
        "\t\telse:\n",
        "\t\t\td = 2**dataWidth - num\n",
        "\treturn d\n",
        "\n",
        "def genWaitAndBias(dataWidth,weightFracWidth,biasFracWidth,inputFile):\n",
        "\tweightIntWidth = dataWidth-weightFracWidth\n",
        "\tbiasIntWidth = dataWidth-biasFracWidth\n",
        "\tmyDataFile = open(inputFile,\"r\")\n",
        "\tweightHeaderFile = open(headerPath+\"weightValues.h\",\"w\")\n",
        "\tmyData = myDataFile.read()\n",
        "\tmyDict = json.loads(myData)\n",
        "\tmyWeights = myDict['weights']\n",
        "\tmyBiases = myDict['biases']\n",
        "\tweightHeaderFile.write(\"int weightValues[]={\")\n",
        "\tfor layer in range(0,len(myWeights)):\n",
        "\t\tfor neuron in range(0,len(myWeights[layer])):\n",
        "\t\t\tfi = 'w_'+str(layer+1)+'_'+str(neuron)+'.mif'\n",
        "\t\t\tf = open(outputPath+fi,'w')\n",
        "\t\t\tfor weight in range(0,len(myWeights[layer][neuron])):\n",
        "\t\t\t\tif 'e' in str(myWeights[layer][neuron][weight]):\n",
        "\t\t\t\t\tp = '0'\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tif myWeights[layer][neuron][weight] > 2**(weightIntWidth-1):\n",
        "\t\t\t\t\t\tmyWeights[layer][neuron][weight] = 2**(weightIntWidth-1)-2**(-weightFracWidth)\n",
        "\t\t\t\t\telif myWeights[layer][neuron][weight] < -2**(weightIntWidth-1):\n",
        "\t\t\t\t\t\tmyWeights[layer][neuron][weight] = -2**(weightIntWidth-1)\n",
        "\t\t\t\t\twInDec = DtoB(myWeights[layer][neuron][weight],dataWidth,weightFracWidth)\n",
        "\t\t\t\t\tp = bin(wInDec)[2:]\n",
        "\t\t\t\tf.write(p+'\\n')\n",
        "\t\t\t\tweightHeaderFile.write(str(wInDec)+',')\n",
        "\t\t\tf.close()\n",
        "\tweightHeaderFile.write('0};\\n')\n",
        "\tweightHeaderFile.close()\n",
        "\n",
        "\tbiasHeaderFile = open(headerPath+\"biasValues.h\",\"w\")\n",
        "\tbiasHeaderFile.write(\"int biasValues[]={\")\n",
        "\tfor layer in range(0,len(myBiases)):\n",
        "\t\tfor neuron in range(0,len(myBiases[layer])):\n",
        "\t\t\tfi = 'b_'+str(layer+1)+'_'+str(neuron)+'.mif'\n",
        "\t\t\tp = myBiases[layer][neuron][0]\n",
        "\t\t\tif 'e' in str(p): #To remove very small values with exponents\n",
        "\t\t\t\tres = '0'\n",
        "\t\t\telse:\n",
        "\t\t\t\tif p > 2**(biasIntWidth-1):\n",
        "\t\t\t\t\tp = 2**(biasIntWidth-1)-2**(-biasFracWidth)\n",
        "\t\t\t\telif p < -2**(biasIntWidth-1):\n",
        "\t\t\t\t\tp = -2**(biasIntWidth-1)\n",
        "\t\t\t\tbInDec = DtoB(p,dataWidth,biasFracWidth)\n",
        "\t\t\t\tres = bin(bInDec)[2:]\n",
        "\t\t\tf = open(outputPath+fi,'w')\n",
        "\t\t\tf.write(res)\n",
        "\t\t\tbiasHeaderFile.write(str(bInDec)+',')\n",
        "\t\t\tf.close()\n",
        "\tbiasHeaderFile.write('0};\\n')\n",
        "\tbiasHeaderFile.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\tgenWaitAndBias(dataWidth,weightFracWidth,biasFracWidth,inputFile)"
      ],
      "metadata": {
        "id": "6Zv_CYhuSkmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating Test Data\n",
        "import sys\n",
        "\n",
        "outputPath = \"./testData/\" #Manually create this folder\n",
        "headerFilePath = \"./testData/\"\n",
        "\n",
        "try:\n",
        "    import cPickle as pickle\n",
        "except:\n",
        "    import pickle\n",
        "import gzip\n",
        "import numpy as np\n",
        "\n",
        "dataWidth = 16                    #specify the number of bits in test data\n",
        "IntSize = 1 #Number of bits of integer portion including sign bit\n",
        "\n",
        "try:\n",
        "    testDataNum = int(sys.argv[1])\n",
        "except:\n",
        "    testDataNum = 3\n",
        "\n",
        "# PyTorch default MNIST normalization constants\n",
        "MNIST_MEAN = 0.1307\n",
        "MNIST_STD = 0.3081\n",
        "\n",
        "def DtoB(num,dataWidth,fracBits):                        #funtion for converting into two's complement format\n",
        "    if num >= 0:\n",
        "        num = num * (2**fracBits)\n",
        "        d = int(num)\n",
        "    else:\n",
        "        num = -num\n",
        "        num = num * (2**fracBits)        #number of fractional bits\n",
        "        num = int(num)\n",
        "        if num == 0:\n",
        "            d = 0\n",
        "        else:\n",
        "            d = 2**dataWidth - num\n",
        "    return d\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    f = gzip.open('mnist.pkl.gz', 'rb')         #change this location to the resiprositry where MNIST dataset sits\n",
        "    try:\n",
        "        training_data, validation_data, test_data = pickle.load(f,encoding='latin1')\n",
        "    except:\n",
        "        training_data, validation_data, test_data = pickle.load(f)\n",
        "    f.close()\n",
        "    return (training_data, validation_data, test_data)\n",
        "\n",
        "def genTestData(dataWidth,IntSize,testDataNum):\n",
        "    dataHeaderFile = open(headerFilePath+\"dataValues.h\",\"w\")\n",
        "    dataHeaderFile.write(\"int dataValues[]={\")\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "\n",
        "\n",
        "    # Normalize like PyTorch\n",
        "    normalized_inputs = (te_d[0] - MNIST_MEAN) / MNIST_STD\n",
        "    test_inputs = [np.reshape(x, (1, 784)) for x in normalized_inputs]\n",
        "\n",
        "    #test_inputs = [np.reshape(x, (1, 784)) for x in te_d[0]]\n",
        "    x = len(test_inputs[0][0])\n",
        "    d=dataWidth-IntSize\n",
        "    count = 0\n",
        "    fileName = 'test_data.txt'\n",
        "    f = open(outputPath+fileName,'w')\n",
        "    fileName = 'visual_data'+str(te_d[1][testDataNum])+'.txt'\n",
        "    g = open(outputPath+fileName,'w')\n",
        "    k = open('testData.txt','w')\n",
        "    for i in range(0,x):\n",
        "        k.write(str(test_inputs[testDataNum][0][i])+',')\n",
        "        dInDec = DtoB(test_inputs[testDataNum][0][i],dataWidth,d)\n",
        "        myData = bin(dInDec)[2:]\n",
        "        dataHeaderFile.write(str(dInDec)+',')\n",
        "        f.write(myData+'\\n')\n",
        "        if test_inputs[testDataNum][0][i]>0:\n",
        "            g.write(str(1)+' ')\n",
        "        else:\n",
        "            g.write(str(0)+' ')\n",
        "        count += 1\n",
        "        if count%28 == 0:\n",
        "            g.write('\\n')\n",
        "    k.close()\n",
        "    g.close()\n",
        "    f.close()\n",
        "    dataHeaderFile.write('0};\\n')\n",
        "    dataHeaderFile.write('int result='+str(te_d[1][testDataNum])+';\\n')\n",
        "    dataHeaderFile.close()\n",
        "\n",
        "\n",
        "def genAllTestData(dataWidth,IntSize):\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "\n",
        "    # Normalize like PyTorch\n",
        "    normalized_inputs = (te_d[0] - MNIST_MEAN) / MNIST_STD\n",
        "    test_inputs = [np.reshape(x, (1, 784)) for x in normalized_inputs]\n",
        "\n",
        "\n",
        "   # test_inputs = [np.reshape(x, (1, 784)) for x in te_d[0]]\n",
        "    x = len(test_inputs[0][0])\n",
        "    d=dataWidth-IntSize\n",
        "    for i in range(len(test_inputs)):\n",
        "        if i < 10:\n",
        "            ext = \"000\"+str(i)\n",
        "        elif i < 100:\n",
        "            ext = \"00\"+str(i)\n",
        "        elif i < 1000:\n",
        "            ext = \"0\"+str(i)\n",
        "        else:\n",
        "            ext = str(i)\n",
        "        fileName = 'test_data_'+ext+'.txt'\n",
        "        f = open(outputPath+fileName,'w')\n",
        "        for j in range(0,x):\n",
        "            dInDec = DtoB(test_inputs[i][0][j],dataWidth,d)\n",
        "            myData = bin(dInDec)[2:]\n",
        "            f.write(myData+'\\n')\n",
        "        f.write(bin(DtoB((te_d[1][i]),dataWidth,0))[2:])\n",
        "        f.close()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #genTestData(dataWidth,IntSize,testDataNum=1)\n",
        "    genAllTestData(dataWidth,IntSize)"
      ],
      "metadata": {
        "id": "ruol5jACVejn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ru7HO4tMweA",
        "outputId": "516f46bc-9e68-4885-80ed-2be404dda859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242,  0.2249,  1.5996,  2.7960,  1.5996,  0.2122, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "            0.1867,  2.6051,  2.7833,  2.7833,  2.7833,  2.5924, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.2631,\n",
              "            2.4651,  2.7960,  2.7833,  2.6178,  2.5415,  2.7833,  0.3013,\n",
              "           -0.3478, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.2969,  0.3395,  2.4269,\n",
              "            2.7833,  2.7960,  2.7833,  2.1469,  0.6450,  2.7833,  2.7960,\n",
              "            1.1286, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242,  1.6505,  2.7833,  2.7833,\n",
              "            2.7833,  2.7960,  2.7833,  2.7833,  0.7977,  1.9814,  2.7960,\n",
              "            1.7014, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242,  0.2249,  2.6051,  2.7960,  2.7960,\n",
              "            1.9942,  1.0268,  2.7960,  2.4778,  0.1740,  0.5813,  2.8215,\n",
              "            1.7141, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242,  0.1867,  2.6051,  2.7833,  2.7833,  1.8541,\n",
              "           -0.2715,  0.5304,  1.1159, -0.1569, -0.4242, -0.4242,  2.7960,\n",
              "            2.6687,  0.2122, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242,  0.0595,  1.6759,  2.7960,  2.5415,  2.2233,  0.6450,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.7960,\n",
              "            2.7833,  1.6759, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.3351,  1.8414,  2.7833,  2.6306,  0.4795, -0.1824, -0.0678,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.7960,\n",
              "            2.7833,  2.0578, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "            0.3013,  2.7833,  2.7833,  0.3777, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.7960,\n",
              "            2.7833,  2.0578, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "            2.0960,  2.7960,  1.9942, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.8215,\n",
              "            2.7960,  2.0705, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.5431,\n",
              "            2.7069,  2.7833,  1.0013, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.7960,\n",
              "            2.7833,  1.4596, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6577,\n",
              "            2.7833,  2.5033, -0.1060, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.3351,  1.2941,  2.7960,\n",
              "            1.9432, -0.2715, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6577,\n",
              "            2.7833,  2.4142, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.3351,  1.2432,  2.7833,  2.4396,\n",
              "            0.4795, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6577,\n",
              "            2.7833,  1.4214, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242,  0.1867,  1.6759,  2.7833,  1.7778, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6704,\n",
              "            2.7960,  2.4396, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242,  1.0268,  2.6051,  2.7960,  1.6378, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6577,\n",
              "            2.7833,  2.7451,  1.4341,  0.1867, -0.0551,  0.6577,  1.8414,\n",
              "            2.4396,  2.7960,  2.4142,  1.7014,  0.2886, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6577,\n",
              "            2.7833,  2.7833,  2.7833,  2.4906,  2.3124,  2.7833,  2.7833,\n",
              "            2.7833,  2.0705,  1.2305, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.0678,\n",
              "            2.1087,  2.7833,  2.7833,  2.7960,  2.7833,  2.7833,  2.5415,\n",
              "            1.4214, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.1060,  1.2050,  2.7833,  2.7960,  2.7833,  1.3705,  0.0467,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
              "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
              "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]]),\n",
              " 0)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# Generate Verilog neuron instantiations automatically\n",
        "# Example: python generate_neurons.py > neurons_inst.vh\n",
        "# ===============================================================\n",
        "\n",
        "num_neurons = 10  # change this to any number of neurons you need\n",
        "layer_num = 2       # layer index (used in w_<layer>_<neuron>.mif)\n",
        "indent = \" \" * 8    # indentation for readability\n",
        "\n",
        "for i in range(num_neurons):\n",
        "    print(f\"neuron #(\")\n",
        "    print(f\"{indent}.numWeight(numWeight),\")\n",
        "    print(f\"{indent}.layerNo(layerNum),\")\n",
        "    print(f\"{indent}.neuronNo({i}),\")\n",
        "    print(f\"{indent}.dataWidth(dataWidth),\")\n",
        "    print(f\"{indent}.sigmoidSize(sigmoidSize),\")\n",
        "    print(f\"{indent}.weightIntWidth(weightIntWidth),\")\n",
        "    print(f\"{indent}.actType(actType),\")\n",
        "    print(f\"{indent}.weightFile(\\\"w_{layer_num}_{i}.mif\\\"),\")\n",
        "    print(f\"{indent}.biasFile(\\\"b_{layer_num}_{i}.mif\\\")\")\n",
        "    print(f\") n_{i}(\")\n",
        "    print(f\"{indent}.clk(clk),\")\n",
        "    print(f\"{indent}.rst(rst),\")\n",
        "    print(f\"{indent}.myinput(x_in),\")\n",
        "    print(f\"{indent}.weightValid(weightValid),\")\n",
        "    print(f\"{indent}.biasValid(biasValid),\")\n",
        "    print(f\"{indent}.weightValue(weightValue),\")\n",
        "    print(f\"{indent}.biasValue(biasValue),\")\n",
        "    print(f\"{indent}.config_layer_num(config_layer_num),\")\n",
        "    print(f\"{indent}.config_neuron_num(config_neuron_num),\")\n",
        "    print(f\"{indent}.myinputValid(x_valid),\")\n",
        "    print(f\"{indent}.out(x_out[{i}*dataWidth+:dataWidth]),\")\n",
        "    print(f\"{indent}.outvalid(o_valid[{i}])\")\n",
        "    print(f\");\")"
      ],
      "metadata": {
        "id": "Nb2t7eqBMnrf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bf73e86-248f-4976-b221-b34ef0a9425d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neuron #(\n",
            "        .numWeight(numWeight),\n",
            "        .layerNo(layerNum),\n",
            "        .neuronNo(0),\n",
            "        .dataWidth(dataWidth),\n",
            "        .sigmoidSize(sigmoidSize),\n",
            "        .weightIntWidth(weightIntWidth),\n",
            "        .actType(actType),\n",
            "        .weightFile(\"w_2_0.mif\"),\n",
            "        .biasFile(\"b_2_0.mif\")\n",
            ") n_0(\n",
            "        .clk(clk),\n",
            "        .rst(rst),\n",
            "        .myinput(x_in),\n",
            "        .weightValid(weightValid),\n",
            "        .biasValid(biasValid),\n",
            "        .weightValue(weightValue),\n",
            "        .biasValue(biasValue),\n",
            "        .config_layer_num(config_layer_num),\n",
            "        .config_neuron_num(config_neuron_num),\n",
            "        .myinputValid(x_valid),\n",
            "        .out(x_out[0*dataWidth+:dataWidth]),\n",
            "        .outvalid(o_valid[0])\n",
            ");\n",
            "neuron #(\n",
            "        .numWeight(numWeight),\n",
            "        .layerNo(layerNum),\n",
            "        .neuronNo(1),\n",
            "        .dataWidth(dataWidth),\n",
            "        .sigmoidSize(sigmoidSize),\n",
            "        .weightIntWidth(weightIntWidth),\n",
            "        .actType(actType),\n",
            "        .weightFile(\"w_2_1.mif\"),\n",
            "        .biasFile(\"b_2_1.mif\")\n",
            ") n_1(\n",
            "        .clk(clk),\n",
            "        .rst(rst),\n",
            "        .myinput(x_in),\n",
            "        .weightValid(weightValid),\n",
            "        .biasValid(biasValid),\n",
            "        .weightValue(weightValue),\n",
            "        .biasValue(biasValue),\n",
            "        .config_layer_num(config_layer_num),\n",
            "        .config_neuron_num(config_neuron_num),\n",
            "        .myinputValid(x_valid),\n",
            "        .out(x_out[1*dataWidth+:dataWidth]),\n",
            "        .outvalid(o_valid[1])\n",
            ");\n",
            "neuron #(\n",
            "        .numWeight(numWeight),\n",
            "        .layerNo(layerNum),\n",
            "        .neuronNo(2),\n",
            "        .dataWidth(dataWidth),\n",
            "        .sigmoidSize(sigmoidSize),\n",
            "        .weightIntWidth(weightIntWidth),\n",
            "        .actType(actType),\n",
            "        .weightFile(\"w_2_2.mif\"),\n",
            "        .biasFile(\"b_2_2.mif\")\n",
            ") n_2(\n",
            "        .clk(clk),\n",
            "        .rst(rst),\n",
            "        .myinput(x_in),\n",
            "        .weightValid(weightValid),\n",
            "        .biasValid(biasValid),\n",
            "        .weightValue(weightValue),\n",
            "        .biasValue(biasValue),\n",
            "        .config_layer_num(config_layer_num),\n",
            "        .config_neuron_num(config_neuron_num),\n",
            "        .myinputValid(x_valid),\n",
            "        .out(x_out[2*dataWidth+:dataWidth]),\n",
            "        .outvalid(o_valid[2])\n",
            ");\n",
            "neuron #(\n",
            "        .numWeight(numWeight),\n",
            "        .layerNo(layerNum),\n",
            "        .neuronNo(3),\n",
            "        .dataWidth(dataWidth),\n",
            "        .sigmoidSize(sigmoidSize),\n",
            "        .weightIntWidth(weightIntWidth),\n",
            "        .actType(actType),\n",
            "        .weightFile(\"w_2_3.mif\"),\n",
            "        .biasFile(\"b_2_3.mif\")\n",
            ") n_3(\n",
            "        .clk(clk),\n",
            "        .rst(rst),\n",
            "        .myinput(x_in),\n",
            "        .weightValid(weightValid),\n",
            "        .biasValid(biasValid),\n",
            "        .weightValue(weightValue),\n",
            "        .biasValue(biasValue),\n",
            "        .config_layer_num(config_layer_num),\n",
            "        .config_neuron_num(config_neuron_num),\n",
            "        .myinputValid(x_valid),\n",
            "        .out(x_out[3*dataWidth+:dataWidth]),\n",
            "        .outvalid(o_valid[3])\n",
            ");\n",
            "neuron #(\n",
            "        .numWeight(numWeight),\n",
            "        .layerNo(layerNum),\n",
            "        .neuronNo(4),\n",
            "        .dataWidth(dataWidth),\n",
            "        .sigmoidSize(sigmoidSize),\n",
            "        .weightIntWidth(weightIntWidth),\n",
            "        .actType(actType),\n",
            "        .weightFile(\"w_2_4.mif\"),\n",
            "        .biasFile(\"b_2_4.mif\")\n",
            ") n_4(\n",
            "        .clk(clk),\n",
            "        .rst(rst),\n",
            "        .myinput(x_in),\n",
            "        .weightValid(weightValid),\n",
            "        .biasValid(biasValid),\n",
            "        .weightValue(weightValue),\n",
            "        .biasValue(biasValue),\n",
            "        .config_layer_num(config_layer_num),\n",
            "        .config_neuron_num(config_neuron_num),\n",
            "        .myinputValid(x_valid),\n",
            "        .out(x_out[4*dataWidth+:dataWidth]),\n",
            "        .outvalid(o_valid[4])\n",
            ");\n",
            "neuron #(\n",
            "        .numWeight(numWeight),\n",
            "        .layerNo(layerNum),\n",
            "        .neuronNo(5),\n",
            "        .dataWidth(dataWidth),\n",
            "        .sigmoidSize(sigmoidSize),\n",
            "        .weightIntWidth(weightIntWidth),\n",
            "        .actType(actType),\n",
            "        .weightFile(\"w_2_5.mif\"),\n",
            "        .biasFile(\"b_2_5.mif\")\n",
            ") n_5(\n",
            "        .clk(clk),\n",
            "        .rst(rst),\n",
            "        .myinput(x_in),\n",
            "        .weightValid(weightValid),\n",
            "        .biasValid(biasValid),\n",
            "        .weightValue(weightValue),\n",
            "        .biasValue(biasValue),\n",
            "        .config_layer_num(config_layer_num),\n",
            "        .config_neuron_num(config_neuron_num),\n",
            "        .myinputValid(x_valid),\n",
            "        .out(x_out[5*dataWidth+:dataWidth]),\n",
            "        .outvalid(o_valid[5])\n",
            ");\n",
            "neuron #(\n",
            "        .numWeight(numWeight),\n",
            "        .layerNo(layerNum),\n",
            "        .neuronNo(6),\n",
            "        .dataWidth(dataWidth),\n",
            "        .sigmoidSize(sigmoidSize),\n",
            "        .weightIntWidth(weightIntWidth),\n",
            "        .actType(actType),\n",
            "        .weightFile(\"w_2_6.mif\"),\n",
            "        .biasFile(\"b_2_6.mif\")\n",
            ") n_6(\n",
            "        .clk(clk),\n",
            "        .rst(rst),\n",
            "        .myinput(x_in),\n",
            "        .weightValid(weightValid),\n",
            "        .biasValid(biasValid),\n",
            "        .weightValue(weightValue),\n",
            "        .biasValue(biasValue),\n",
            "        .config_layer_num(config_layer_num),\n",
            "        .config_neuron_num(config_neuron_num),\n",
            "        .myinputValid(x_valid),\n",
            "        .out(x_out[6*dataWidth+:dataWidth]),\n",
            "        .outvalid(o_valid[6])\n",
            ");\n",
            "neuron #(\n",
            "        .numWeight(numWeight),\n",
            "        .layerNo(layerNum),\n",
            "        .neuronNo(7),\n",
            "        .dataWidth(dataWidth),\n",
            "        .sigmoidSize(sigmoidSize),\n",
            "        .weightIntWidth(weightIntWidth),\n",
            "        .actType(actType),\n",
            "        .weightFile(\"w_2_7.mif\"),\n",
            "        .biasFile(\"b_2_7.mif\")\n",
            ") n_7(\n",
            "        .clk(clk),\n",
            "        .rst(rst),\n",
            "        .myinput(x_in),\n",
            "        .weightValid(weightValid),\n",
            "        .biasValid(biasValid),\n",
            "        .weightValue(weightValue),\n",
            "        .biasValue(biasValue),\n",
            "        .config_layer_num(config_layer_num),\n",
            "        .config_neuron_num(config_neuron_num),\n",
            "        .myinputValid(x_valid),\n",
            "        .out(x_out[7*dataWidth+:dataWidth]),\n",
            "        .outvalid(o_valid[7])\n",
            ");\n",
            "neuron #(\n",
            "        .numWeight(numWeight),\n",
            "        .layerNo(layerNum),\n",
            "        .neuronNo(8),\n",
            "        .dataWidth(dataWidth),\n",
            "        .sigmoidSize(sigmoidSize),\n",
            "        .weightIntWidth(weightIntWidth),\n",
            "        .actType(actType),\n",
            "        .weightFile(\"w_2_8.mif\"),\n",
            "        .biasFile(\"b_2_8.mif\")\n",
            ") n_8(\n",
            "        .clk(clk),\n",
            "        .rst(rst),\n",
            "        .myinput(x_in),\n",
            "        .weightValid(weightValid),\n",
            "        .biasValid(biasValid),\n",
            "        .weightValue(weightValue),\n",
            "        .biasValue(biasValue),\n",
            "        .config_layer_num(config_layer_num),\n",
            "        .config_neuron_num(config_neuron_num),\n",
            "        .myinputValid(x_valid),\n",
            "        .out(x_out[8*dataWidth+:dataWidth]),\n",
            "        .outvalid(o_valid[8])\n",
            ");\n",
            "neuron #(\n",
            "        .numWeight(numWeight),\n",
            "        .layerNo(layerNum),\n",
            "        .neuronNo(9),\n",
            "        .dataWidth(dataWidth),\n",
            "        .sigmoidSize(sigmoidSize),\n",
            "        .weightIntWidth(weightIntWidth),\n",
            "        .actType(actType),\n",
            "        .weightFile(\"w_2_9.mif\"),\n",
            "        .biasFile(\"b_2_9.mif\")\n",
            ") n_9(\n",
            "        .clk(clk),\n",
            "        .rst(rst),\n",
            "        .myinput(x_in),\n",
            "        .weightValid(weightValid),\n",
            "        .biasValid(biasValid),\n",
            "        .weightValue(weightValue),\n",
            "        .biasValue(biasValue),\n",
            "        .config_layer_num(config_layer_num),\n",
            "        .config_neuron_num(config_neuron_num),\n",
            "        .myinputValid(x_valid),\n",
            "        .out(x_out[9*dataWidth+:dataWidth]),\n",
            "        .outvalid(o_valid[9])\n",
            ");\n"
          ]
        }
      ]
    }
  ]
}